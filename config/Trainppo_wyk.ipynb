{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 使用 Gymnasium\n",
      "============================================================================================\n",
      "Device set to : NVIDIA GeForce RTX 3060\n",
      "============================================================================================\n"
     ]
    }
   ],
   "source": [
    "from gym_interface import Agent, State\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import socket\n",
    "import time\n",
    "from collections import deque, namedtuple\n",
    "from typing import Dict, Iterable, List, Literal, Optional, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from distutils.util import strtobool\n",
    "# from rlmodel.utils.utils import print_args, print_box, connected_to_internet\n",
    "import wandb\n",
    "import setproctitle\n",
    "from pathlib import Path\n",
    "\n",
    "import os, sys\n",
    "from PPO import PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*自定义处理函数*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_Input(action: int) -> str:\n",
    "    # example:\n",
    "    if action == -1:\n",
    "        return \"\"\n",
    "\n",
    "    TacticID = f'<AgentCMD><uint64_t>{action}</uint64_t></AgentCMD>'\n",
    "    s = '<c>' + TacticID + '</c>'\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_Output(state: Dict[str, any]) -> dict:\n",
    "    #example:\n",
    "    tmp = []\n",
    "    Input = {}\n",
    "    for input in state:\n",
    "        for k, v in input.items():\n",
    "            if k == 'PlaneInfo':\n",
    "                tmp.append(v)\n",
    "            else:\n",
    "                Input[k] = v\n",
    "    Input['PlaneInfo'] = tmp\n",
    "    return Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputToTensor(state: Dict[str, any]) -> np.array:\n",
    "    self = state['PlaneInfo'][0]['entity_motion_state']\n",
    "    enemy = state['TargetMessage']['entity_motion_state']\n",
    "    self_pitch = self['AttitudeInfo']['pitch']/10\n",
    "    self_yaw = self['AttitudeInfo']['yaw']/10\n",
    "    self_roll = self['AttitudeInfo']['roll']/10\n",
    "    self_lon = self['PositionInfo']['longitude']/10\n",
    "    self_lat = self['PositionInfo']['latitude']/10\n",
    "    self_alt = self['PositionInfo']['altitude']/100\n",
    "    self_vx = self['ECEFVelocity']['vx']/10\n",
    "    self_vy = self['ECEFVelocity']['vy']/10\n",
    "    self_vz = self['ECEFVelocity']['vz']/10\n",
    "    enemy_pitch = enemy['AttitudeInfo']['pitch']/10\n",
    "    enemy_yaw = enemy['AttitudeInfo']['yaw']/10\n",
    "    enemy_roll = enemy['AttitudeInfo']['roll']/10\n",
    "    enemy_lon = enemy['PositionInfo']['longitude']/10\n",
    "    enemy_lat = enemy['PositionInfo']['latitude']/10\n",
    "    enemy_alt = enemy['PositionInfo']['altitude']/100\n",
    "    enemy_vx = enemy['ECEFVelocity']['vx']/10\n",
    "    enemy_vy = enemy['ECEFVelocity']['vy']/10\n",
    "    enemy_vz = enemy['ECEFVelocity']['vz']/10\n",
    "    return np.array([self_pitch, self_yaw, self_roll, self_lon, \n",
    "                     self_lat, self_alt, self_vx, self_vy, self_vz, \n",
    "                     enemy_pitch, enemy_yaw, enemy_roll, enemy_lon, \n",
    "                     enemy_lat, enemy_alt, enemy_vx, enemy_vy, enemy_vz])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_Reward(state:Dict[str, any]):\n",
    "    #example:\n",
    "    global count\n",
    "    if not state[\"PlaneInfo\"]:\n",
    "        return 0\n",
    "    self = state['PlaneInfo'][0]['entity_motion_state']\n",
    "    enemy = state['TargetMessage']['entity_motion_state']\n",
    "    self_lon = self['PositionInfo']['longitude']\n",
    "    self_lat = self['PositionInfo']['latitude']\n",
    "    self_alt = self['PositionInfo']['altitude']/1000\n",
    "    enemy_lon = enemy['PositionInfo']['longitude']\n",
    "    enemy_lat = enemy['PositionInfo']['latitude']\n",
    "    enemy_alt = enemy['PositionInfo']['altitude']/1000\n",
    "    self_state = state['PlaneInfo'][0]['entity_state']\n",
    "    enemy_state = state['TargetMessage']['entity_state']\n",
    "    # dist_to_goal = - np.sqrt(np.max(np.square(self_lon-enemy_lon), 0) + np.max(np.square(self_lat-enemy_lat), 0) + np.max(np.square(self_alt-enemy_alt), 0))\n",
    "    # r = dist_to_goal\n",
    "    r = 0\n",
    "    if(enemy_state == 5):\n",
    "        # print(\"敌死\")\n",
    "        r += 500\n",
    "    elif(self_state == 5):\n",
    "        # print(\"我亡\")\n",
    "        r -= 500\n",
    "    # else:\n",
    "    #     r -= 1\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_Termination(state:Dict[str, any]) -> bool:\n",
    "    #example\n",
    "        self = state['PlaneInfo'][0]['entity_state']\n",
    "        enemy = state['TargetMessage']['entity_state']\n",
    "        if self == 5 or enemy == 5:\n",
    "            return True\n",
    "        else:    \n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_算法参数配置_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "sys.path.append(os.path.abspath(os.getcwd()))\n",
    "num_agents = 1\n",
    "num_enemies = 1\n",
    "episode_length = 100\n",
    "save_interval = 1000\n",
    "log_interval = 10\n",
    "model_dir = (\n",
    "        Path(os.path.dirname(os.path.dirname(os.getcwd())) + \"/results\")\n",
    "        /\"save\"\n",
    "    )\n",
    "all_args = {\n",
    "    \"algorithm_name\": \"ppo\",\n",
    "    \"use_recurrent_policy\": False,\n",
    "    \"use_naive_recurrent_policy\": False,\n",
    "    \"share_policy\": True,\n",
    "    \"use_wandb\": True,\n",
    "    \"seed\": 0,\n",
    "    \"use_centralized_V\": True,\n",
    "    \"use_linear_lr_decay\": True,\n",
    "    \"hidden_size\": 16,\n",
    "    \"recurrent_N\": 1,\n",
    "    \"act_space\": 6,\n",
    "    \"obs_space\": 18,\n",
    "    \"shared_obs_space\": 18*num_agents,\n",
    "    \"model_dir\": None,\n",
    "    \"episode_length\": episode_length,\n",
    "    \"gamma\": 0.98,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"use_gae\": True,\n",
    "    \"clip_param\": 0.2,\n",
    "    \"ppo_epoch\": 15,\n",
    "    \"num_mini_batch\": 1,\n",
    "    \"data_chunk_length\": 10,\n",
    "    \"value_loss_coef\": 0.5,\n",
    "    \"entropy_coef\": 0.01,\n",
    "    \"max_grad_norm\": 10.0,\n",
    "    \"huber_delta\": 10.0,\n",
    "    \"use_max_grad_norm\": True,\n",
    "    \"use_clipped_value_loss\": True,\n",
    "    \"use_huber_loss\": True,\n",
    "    \"use_popart\": True,\n",
    "    \"use_valuenorm\": False,\n",
    "    \"use_value_active_masks\": True,\n",
    "    \"use_policy_active_masks\": True,\n",
    "    \"lr\": 7e-5,\n",
    "    \"critic_lr\": 7e-4,\n",
    "    \"opti_eps\": 1e-5,\n",
    "    \"weight_decay\": 0,\n",
    "    \"gain\": 0.01,\n",
    "    \"use_orthogonal\": True,\n",
    "    \"use_feature_normalization\": True,\n",
    "    \"use_ReLU\": False,\n",
    "    \"stacked_frames\": 1,\n",
    "    \"layer_N\": 1,\n",
    "    \"n_rollout_threads\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "run_dir = (\n",
    "        Path(os.path.dirname(os.path.dirname(os.getcwd())) + \"/results\")\n",
    "        / all_args[\"algorithm_name\"]\n",
    "    )\n",
    "config = {\n",
    "    \"all_args\": all_args,\n",
    "    \"num_agents\": num_agents,\n",
    "    \"num_enemies\":num_enemies,\n",
    "    \"device\": device,\n",
    "    \"run_dir\": run_dir\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*W&B记录训练日志*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Create a W&B account'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Create an account here: https://wandb.ai/authorize?signup=true&ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\15961\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwstybh\u001b[0m (\u001b[33mwstybh-beihang-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\codeproject\\results\\ppo\\wandb\\run-20260102_161218-tczc27s7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wstybh-beihang-university/simplecq/runs/tczc27s7' target=\"_blank\">ppo_seed0</a></strong> to <a href='https://wandb.ai/wstybh-beihang-university/simplecq' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wstybh-beihang-university/simplecq' target=\"_blank\">https://wandb.ai/wstybh-beihang-university/simplecq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wstybh-beihang-university/simplecq/runs/tczc27s7' target=\"_blank\">https://wandb.ai/wstybh-beihang-university/simplecq/runs/tczc27s7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if all_args[\"use_wandb\"]:\n",
    "        # for supercloud when no internet_connection\n",
    "        # if not connected_to_internet():\n",
    "        #     import json\n",
    "\n",
    "        #     # save a json file with your wandb api key in your\n",
    "        #     # home folder as {'my_wandb_api_key': 'INSERT API HERE'}\n",
    "        #     # NOTE this is only for running on systems without internet access\n",
    "        #     # have to run `wandb sync wandb/run_name` to sync logs to wandboard\n",
    "        #     with open(os.path.dirname(os.path.dirname(os.getcwd())) + \"/keys.json\") as json_file:\n",
    "        #         key = json.load(json_file)\n",
    "        #         my_wandb_api_key = key[\"my_wandb_api_key\"]  # NOTE change here as well\n",
    "        #     os.environ[\"WANDB_API_KEY\"] = my_wandb_api_key\n",
    "        #     os.environ[\"WANDB_MODE\"] = \"dryrun\"\n",
    "        #     os.environ[\"WANDB_SAVE_CODE\"] = \"true\"\n",
    "\n",
    "        # print_box(\"Creating wandboard...\")\n",
    "        run = wandb.init(\n",
    "            config=all_args,\n",
    "            project=\"simplecq\",\n",
    "            # project=all_args.env_name,\n",
    "            # entity=\"cc\",\n",
    "            notes=socket.gethostname(),\n",
    "            name=str(all_args[\"algorithm_name\"])\n",
    "            + \"_seed\"\n",
    "            + str(all_args[\"seed\"]),\n",
    "            # group=all_args.scenario_name,\n",
    "            dir=str(run_dir),\n",
    "            # job_type=\"training\",\n",
    "            reinit='return_previous',\n",
    "        )\n",
    "        \n",
    "setproctitle.setproctitle(\n",
    "        str(all_args[\"algorithm_name\"])\n",
    "        + \"@\"\n",
    "        + str(\"lapluma030\")\n",
    "    )\n",
    "\n",
    "# seed\n",
    "torch.manual_seed(all_args[\"seed\"])\n",
    "torch.cuda.manual_seed_all(all_args[\"seed\"])\n",
    "np.random.seed(all_args[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._dynamo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m is_continuous_action_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# 离散动作空间\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 初始化PPO\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m ppo_agent \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_actor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_actor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_critic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_critic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mK_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps_clipping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps_clipping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_continuous_action_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_continuous_action_space\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\codeproject\\simplecq\\config\\PPO.py:152\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[1;34m(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clipping, is_continuous_action_space, action_std_init)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m RolloutBuffer()\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m ActorCritic(state_dim, action_dim, is_continuous_action_space, action_std_init)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_actor\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_critic\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_old \u001b[38;5;241m=\u001b[39m ActorCritic(state_dim, action_dim, is_continuous_action_space, action_std_init)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_old\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mstate_dict())\n",
      "File \u001b[1;32md:\\anaconda\\envs\\simplecq_torch\\lib\\site-packages\\torch\\optim\\adam.py:78\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     66\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     67\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m     68\u001b[0m     betas\u001b[38;5;241m=\u001b[39mbetas,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m     fused\u001b[38;5;241m=\u001b[39mfused,\n\u001b[0;32m     77\u001b[0m )\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\simplecq_torch\\lib\\site-packages\\torch\\optim\\optimizer.py:371\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    368\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: param_groups}]\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[1;32m--> 371\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\simplecq_torch\\lib\\site-packages\\torch\\_compile.py:27\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m disable_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dynamo_disable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m disable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[0;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch._dynamo'"
     ]
    }
   ],
   "source": [
    "# PPO参数设置\n",
    "state_dim = 18  # 状态维度\n",
    "action_dim = 6  # 动作维度\n",
    "lr_actor = 0.0003  # Actor学习率\n",
    "lr_critic = 0.001  # Critic学习率\n",
    "gamma = 0.99  # 折扣因子\n",
    "K_epochs = 5  # 每次更新的训练轮数\n",
    "eps_clipping = 0.2  # PPO裁剪参数\n",
    "is_continuous_action_space = False  # 离散动作空间\n",
    "\n",
    "# 初始化PPO\n",
    "ppo_agent = PPO(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    lr_actor=lr_actor,\n",
    "    lr_critic=lr_critic,\n",
    "    gamma=gamma,\n",
    "    K_epochs=K_epochs,\n",
    "    eps_clipping=eps_clipping,\n",
    "    is_continuous_action_space=is_continuous_action_space\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*端口及输出类型指定*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 40029\n",
    "outputs_type = \"uint64_t\"\n",
    "# outputs_type = {\n",
    "#     \"AgentCMD1\": \"uint64_t\",\n",
    "#     \"AgentCMD2\": \"uint64_t\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(port=port, \n",
    "              outputs_type=outputs_type,\n",
    "              process_input=parse_Input,\n",
    "              process_output=parse_Output,\n",
    "              reward_func=cal_Reward,\n",
    "              end_func=cal_Termination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = agent.reset()\n",
    "# a, b, c, _ = agent.step(0)\n",
    "# print(s)\n",
    "# print(a)\n",
    "# print(b)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义好奇心模型\n",
    "class CuriosityModule(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=128):\n",
    "        super(CuriosityModule, self).__init__()\n",
    "        self.state_action_encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.state_predictor = nn.Linear(hidden_size, state_dim)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # 将状态和动作拼接\n",
    "        state_action = torch.cat([state, action], dim=-1)\n",
    "        encoded = self.state_action_encoder(state_action)\n",
    "        predicted_next_state = self.state_predictor(encoded)\n",
    "        return predicted_next_state\n",
    "\n",
    "# 初始化好奇心模块\n",
    "curiosity_model = CuriosityModule(state_dim=18, action_dim=6)\n",
    "curiosity_optimizer = optim.Adam(curiosity_model.parameters(), lr=1e-4)\n",
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*训练函数*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第三部分：修改train函数\n",
    "def train(env: Agent, episodes, enable_log=True):\n",
    "    Win = 0\n",
    "    Lose = 0\n",
    "    Tie = 0\n",
    "    # 时间间隔\n",
    "    time_interval = 20\n",
    "    decision_interval = 100\n",
    "    init_val = env.reset()\n",
    "    print(\"init_val:\", init_val)\n",
    "    \n",
    "    global count\n",
    "    count = 0\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        beat = 0\n",
    "        defeat = 0\n",
    "        episode_rewards = 0\n",
    "        \n",
    "        # 清空PPO缓冲区准备新一轮的收集\n",
    "        ppo_agent.buffer.clear()\n",
    "        current_state = np.array([0.0, 0.0, 0.0, 11.5, 3.5, 25.0, \n",
    "                                  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, \n",
    "                                  0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "        # 收集状态和动作数据\n",
    "        state_action_pairs = []\n",
    "        for step in range(episode_length*time_interval):\n",
    "            # 每隔特定步数执行动作决策\n",
    "            if (step % (decision_interval-1)) == 0:\n",
    "                action_idx = ppo_agent.select_action(current_state)                \n",
    "                # 可用战术列表\n",
    "                TacticList = [1, 2, 14, 27, 30, 32]\n",
    "                \n",
    "                # 执行动作并获取环境反馈\n",
    "                action = TacticList[action_idx]\n",
    "                obs, rew, done, _ = env.step(action)\n",
    "                 # 将新状态转换为tensor\n",
    "                current_state = outputToTensor(obs)\n",
    "\n",
    "                os.makedirs(\"D:/A_code/simplecq/record\", exist_ok=True)\n",
    "                # 记录当前状态current_state和动作action到record路径下的action.txt文件\n",
    "                with open(\"D:/A_code/simplecq/record/action.txt\", \"a\") as f:\n",
    "                    state_str = ' '.join(map(str, current_state))\n",
    "                    f.write(f\"{state_str} {action}\\n\")\n",
    "                    # f.write(f\"Episode {episode}, Step {step}: Action {TacticList[action_idx]}\\n\")                \n",
    "                ###\n",
    "                # 将当前状态和动作转换为张量\n",
    "                # 确保 state_tensor 的形状正确\n",
    "                state_tensor = torch.tensor(current_state, dtype=torch.float32).unsqueeze(0)  # (1, 18)\n",
    "\n",
    "                # 将 action_idx 转换为独热编码\n",
    "                action_one_hot = torch.zeros(action_dim, dtype=torch.float32)\n",
    "                action_one_hot[action_idx] = 1.0\n",
    "                action_tensor = action_one_hot.unsqueeze(0)  # (1, 6)\n",
    "\n",
    "                # 拼接后输入模型\n",
    "                predicted_next_state = curiosity_model(state_tensor, action_tensor)\n",
    "\n",
    "                # 计算预测误差（好奇心奖励）\n",
    "                actual_next_state = torch.tensor(current_state, dtype=torch.float32).unsqueeze(0)\n",
    "                curiosity_reward = mse_loss(predicted_next_state, actual_next_state).item()\n",
    "\n",
    "                # 将好奇心奖励加入总奖励\n",
    "                curiosity_weight = 0.1  # 好奇心奖励的权重\n",
    "                total_reward = rew - curiosity_weight * curiosity_reward\n",
    "\n",
    "                # 记录奖励\n",
    "                ppo_agent.buffer.rewards.append(total_reward)\n",
    "                ###\n",
    "                # 记录奖励和终止状态到PPO缓冲区\n",
    "                # ppo_agent.buffer.rewards.append(rew)\n",
    "                ppo_agent.buffer.is_terminals.append(done)\n",
    "                \n",
    "                # episode_rewards += rew\n",
    "                episode_rewards += total_reward\n",
    "                \n",
    "                if done or count > 10000:\n",
    "                    count = 0\n",
    "                    env.reset()\n",
    "                    current_state = np.array([0.0,0.0,0.0, 11.5, 3.5, 25.0, \n",
    "                                  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, \n",
    "                                  0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "            else:\n",
    "                # 执行默认动作\n",
    "                obs, rew, done, _ = env.step(action=2)\n",
    "\n",
    "                if done or count > 10000:\n",
    "                    count = 0\n",
    "                    env.reset()\n",
    "                    current_state = np.array([0.0,0.0,0.0, 11.5, 3.5, 25.0, \n",
    "                                  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, \n",
    "                                  0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "            state_action_pairs.append((current_state, action_idx))\n",
    "            # 状态判断\n",
    "            if obs['PlaneInfo'][0]['entity_state'] == 5:\n",
    "                defeat = 1\n",
    "            if obs['TargetMessage']['entity_state'] == 5:\n",
    "                beat = 1\n",
    "            count += 1\n",
    "        \n",
    "        # 每个回合结束后更新PPO\n",
    "        ppo_agent.update()\n",
    "\n",
    "        # 更新好奇心模型\n",
    "        for state, action in state_action_pairs:\n",
    "            # 确保 state_tensor 的形状正确\n",
    "            state_tensor = torch.tensor(current_state, dtype=torch.float32).unsqueeze(0)  # (1, 18)\n",
    "\n",
    "            # 将 action_idx 转换为独热编码\n",
    "            action_one_hot = torch.zeros(action_dim, dtype=torch.float32)\n",
    "            action_one_hot[action_idx] = 1.0\n",
    "            action_tensor = action_one_hot.unsqueeze(0)  # (1, 6)\n",
    "\n",
    "            # 拼接后输入模型\n",
    "            predicted_next_state = curiosity_model(state_tensor, action_tensor)\n",
    "            actual_next_state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            # 计算损失并优化\n",
    "            loss = mse_loss(predicted_next_state, actual_next_state)\n",
    "            curiosity_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            curiosity_optimizer.step()\n",
    "        \n",
    "        # 更新胜负平记录\n",
    "        if beat == 1: Win += 1\n",
    "        elif beat == 0 and defeat == 1: Lose += 1\n",
    "        else: Tie += 1\n",
    "        \n",
    "        # # 保存模型\n",
    "        # if episode % save_interval == 0 or episode == episodes - 1:\n",
    "        #     checkpoint_path = f\"{str(model_dir)}/ppo_model_{episode}.pth\"\n",
    "        #     ppo_agent.save(checkpoint_path)\n",
    "        # 保存模型\n",
    "        if episode % save_interval == 0 or episode == episodes - 1:\n",
    "            # 确保保存目录存在\n",
    "            os.makedirs(str(model_dir), exist_ok=True)\n",
    "            checkpoint_path = f\"{str(model_dir)}/ppo_model_{episode}.pth\"\n",
    "            try:\n",
    "                ppo_agent.save(checkpoint_path)\n",
    "                print(f\"模型已保存到: {checkpoint_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"保存模型时出错: {e}\")\n",
    "                # 尝试保存到当前目录\n",
    "                alternative_path = f\"./ppo_model_{episode}.pth\"\n",
    "                try:\n",
    "                    ppo_agent.save(alternative_path)\n",
    "                    print(f\"模型已保存到备用路径: {alternative_path}\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"保存到备用路径也失败: {e2}\")\n",
    "                # 记录信息\n",
    "        if episode % log_interval == 0:\n",
    "            print(\n",
    "                f\"平均回合奖励：{episode_rewards:.3f} \\t\"\n",
    "                f\"总步数：{(episode + 1) * episode_length} \\t\"\n",
    "                f\"完成度：{(episode + 1) / episodes * 100:.3f}%\"\n",
    "            )\n",
    "            print(f\"训练进度：胜：{Win} 负：{Lose} 平：{Tie-1}\")\n",
    "            \n",
    "            # 使用wandb记录训练信息\n",
    "            if all_args[\"use_wandb\"]:\n",
    "                wandb.log({\n",
    "                    \"episode_reward\": episode_rewards,\n",
    "                    # # 记录policyloss和valueloss\n",
    "                    # \"policy_loss\": ppo_agent.policy_loss,\n",
    "                    # \"value_loss\": ppo_agent.value_loss,\n",
    "                    \"win_count\": Win,\n",
    "                    \"lose_count\": Lose,\n",
    "                    \"tie_count\": Tie,\n",
    "                    \"win_rate\": Win / (episode + 1)\n",
    "                }, step=(episode + 1) * episode_length)\n",
    "        \n",
    "        if episode % 100 == 0 and enable_log:\n",
    "            print(f\"已完成：{episode / episodes * 100}%\")\n",
    "    \n",
    "    print(f\"训练结束。胜：{Win} 负：{Lose} 平：{Tie-1}\")\n",
    "    \n",
    "    if all_args[\"use_wandb\"]:\n",
    "        run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*运行训练*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(agent, 500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simplecq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
