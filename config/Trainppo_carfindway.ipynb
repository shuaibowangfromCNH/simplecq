{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_interface import Agent, State\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import socket\n",
    "import time\n",
    "from collections import deque, namedtuple\n",
    "from typing import Dict, Iterable, List, Literal, Optional, Union\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from distutils.util import strtobool\n",
    "# from rlmodel.utils.utils import print_args, print_box, connected_to_internet\n",
    "import wandb\n",
    "import setproctitle\n",
    "from pathlib import Path\n",
    "\n",
    "import os, sys\n",
    "from PPO import PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*自定义处理函数*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_Input(action) -> str:\n",
    "    try:\n",
    "        # 直接提取原始动作值\n",
    "        raw_velocity = float(action[0])  # PPO输出的原始速度值\n",
    "        raw_direction = float(action[1])  # PPO输出的原始方向值\n",
    "        \n",
    "        # 在这里进行缩放\n",
    "        # 假设PPO输出范围是 [-1, 1]，缩放到目标范围\n",
    "        velocity = (raw_velocity + 1) * 10  # [-1,1] -> [0,20]\n",
    "        direction = raw_direction * np.pi   # [-1,1] -> [-π, π]\n",
    "        # 确保在合理范围内\n",
    "        velocity = max(0, min(20, velocity))\n",
    "        direction = max(-np.pi, min(np.pi, direction))\n",
    "        \n",
    "        velocity_str = f\"{velocity:.4f}\"\n",
    "        direction_str = f\"{direction:.4f}\"\n",
    "        \n",
    "        cmd = f\"<c><targetVel><float>{velocity_str}</float></targetVel><targetDir><float>{direction_str}</float></targetDir></c>\"\n",
    "        \n",
    "        return cmd\n",
    "        \n",
    "    except:\n",
    "        # 出错时返回零动作\n",
    "        return \"<c><targetVel><float>0.0</float></targetVel><targetDir><float>0.0</float></targetDir></c>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_Output(state: List[Dict[str, any]]) -> dict:\n",
    "    #example:如果要加别的输入数据，思考下如何修改，比如只想获取我车的distance参数。\n",
    "    tmp = []\n",
    "    Input = {}\n",
    "    for input in state:\n",
    "        for k, v in input.items():\n",
    "            if k == 'carInfo':\n",
    "                tmp.append(v)\n",
    "            else:\n",
    "                Input[k] = v\n",
    "    Input['carInfo'] = tmp\n",
    "    return Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_Reward(state:Dict[str, any]) -> float:\n",
    "    \"\"\"\n",
    "    动力学一致、PPO 友好的车辆追踪奖励函数\n",
    "    只依赖状态，不直接使用 action\n",
    "    \"\"\"\n",
    "    if 'carInfo' not in state or len(state['carInfo']) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1. 区分我方 / 敌方\n",
    "    # -----------------------------\n",
    "    if state['carInfo'][0]['baseInfo']['side'] == 1:\n",
    "        self_car = state['carInfo'][0]\n",
    "        enemy_car = state['carInfo'][1]\n",
    "    else:\n",
    "        self_car = state['carInfo'][1]\n",
    "        enemy_car = state['carInfo'][0]\n",
    "\n",
    "    try:\n",
    "        # 位置\n",
    "        sx, sy, sz = self_car['position'].values()\n",
    "        ex, ey, ez = enemy_car['position'].values()\n",
    "\n",
    "        # 速度\n",
    "        svx, svy, svz = self_car['velocity'].values()\n",
    "        evx, evy, evz = enemy_car['velocity'].values()\n",
    "\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "    \n",
    "    # -----------------------------\n",
    "    # 2. 基本物理量\n",
    "    # -----------------------------\n",
    "    dx, dy, dz = ex - sx, ey - sy, ez - sz\n",
    "    distance = math.sqrt(dx*dx + dy*dy + dz*dz)\n",
    "\n",
    "    self_speed = math.sqrt(svx*svx + svy*svy + svz*svz)\n",
    "    enemy_speed = math.sqrt(evx*evx + evy*evy + evz*evz)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3. 全局历史状态（episode 内）\n",
    "    # -----------------------------\n",
    "    global prev_distance, prev_speed, prev_heading\n",
    "\n",
    "    if 'prev_distance' not in globals() or prev_distance is None:\n",
    "        prev_distance = distance\n",
    "    if 'prev_speed' not in globals() or prev_speed is None:\n",
    "        prev_speed = self_speed\n",
    "    if 'prev_heading' not in globals():\n",
    "        prev_heading = math.atan2(svy, svx) if self_speed > 0.1 else 0.0\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4. 距离奖励（主项，平滑）\n",
    "    # -----------------------------\n",
    "    # 使用 log 缩放，避免远距离梯度过小\n",
    "    distance_reward = 0#-math.log(distance / 1000.0 + 1.0)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5. 接近速度奖励（非常关键）\n",
    "    # -----------------------------\n",
    "    distance_change = prev_distance - distance\n",
    "    proximity_reward = 1.00 * distance_change   # 接近为正，远离为负\n",
    "\n",
    "    # -----------------------------\n",
    "    # 6. 航向一致性奖励（基于真实速度）\n",
    "    # -----------------------------\n",
    "    heading_reward = 0.0\n",
    "    if distance > 1.0 and self_speed > 0.1:\n",
    "        to_target = [dx/distance, dy/distance, dz/distance]\n",
    "        velocity_dir = [svx/self_speed, svy/self_speed, svz/self_speed]\n",
    "        cos_angle = sum(to_target[i] * velocity_dir[i] for i in range(3))\n",
    "        heading_reward = 0.5 * max(0.0, cos_angle)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 7. 速度合理性奖励（随距离变化）\n",
    "    # -----------------------------\n",
    "    if distance > 3000:\n",
    "        desired_speed = 15.0\n",
    "    elif distance > 1000:\n",
    "        desired_speed = 10.0\n",
    "    elif distance > 200:\n",
    "        desired_speed = 5.0\n",
    "    else:\n",
    "        desired_speed = 3.0\n",
    "\n",
    "    speed_error = abs(self_speed - desired_speed)\n",
    "    speed_reward = -0.03 * speed_error\n",
    "\n",
    "    # -----------------------------\n",
    "    # 8. 真实加速度惩罚（非常重要）\n",
    "    # -----------------------------\n",
    "    acc = abs(self_speed - prev_speed)\n",
    "    acc_penalty = -0.2 * acc\n",
    "\n",
    "    # -----------------------------\n",
    "    # 9. 真实转向变化惩罚（不是 action）\n",
    "    # -----------------------------\n",
    "    heading_penalty = 0.0\n",
    "    if self_speed > 0.1:\n",
    "        current_heading = math.atan2(svy, svx)\n",
    "        heading_change = abs(current_heading - prev_heading)\n",
    "        if heading_change > math.pi:\n",
    "            heading_change = 2 * math.pi - heading_change\n",
    "        heading_penalty = -0.1 * heading_change\n",
    "    else:\n",
    "        current_heading = prev_heading\n",
    "\n",
    "    # -----------------------------\n",
    "    # 10. 时间惩罚（防止磨蹭）\n",
    "    # -----------------------------\n",
    "    time_penalty = 0 # -0.02\n",
    "\n",
    "    # -----------------------------\n",
    "    # 11. 终端奖励\n",
    "    # -----------------------------\n",
    "    terminal_reward = 0.0\n",
    "    is_terminal = False\n",
    "    if distance < 10.0:\n",
    "        terminal_reward = 100.0\n",
    "        is_terminal = True\n",
    "        print(\"✅ 成功拦截\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 12. 总奖励\n",
    "    # -----------------------------\n",
    "    total_reward = (\n",
    "        distance_reward +\n",
    "        proximity_reward +\n",
    "        heading_reward +\n",
    "        speed_reward +\n",
    "        acc_penalty +\n",
    "        heading_penalty +\n",
    "        time_penalty +\n",
    "        terminal_reward\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 13. 更新历史状态\n",
    "    # -----------------------------\n",
    "    prev_distance = distance\n",
    "    prev_speed = self_speed\n",
    "    prev_heading = current_heading\n",
    "    \n",
    "    # 输出调试信息\n",
    "    if is_terminal or np.random.random() < 0.01:\n",
    "        print(f\"距离奖励：{distance_reward:.3f}, 接近奖励：{proximity_reward:.3f}, 航向奖励：{heading_reward:.3f}, 速度奖励：{speed_reward:.3f}, 加速度惩罚：{acc_penalty:.3f}, 转向惩罚：{heading_penalty:.3f}, 时间惩罚：{time_penalty:.3f}, 终端奖励：{terminal_reward:.3f}, 总奖励：{total_reward:.3f}\")\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_reward_v1_5(state: dict) -> float:\n",
    "    global prev_distance\n",
    "\n",
    "    if 'carInfo' not in state or len(state['carInfo']) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # 区分敌我\n",
    "    if state['carInfo'][0]['baseInfo']['side'] == 1:\n",
    "        self_car = state['carInfo'][0]\n",
    "        enemy_car = state['carInfo'][1]\n",
    "    else:\n",
    "        self_car = state['carInfo'][1]\n",
    "        enemy_car = state['carInfo'][0]\n",
    "\n",
    "    try:\n",
    "        sx, sy, sz = self_car['position']['x'], self_car['position']['y'], self_car['position']['z']\n",
    "        ex, ey, ez = enemy_car['position']['x'], enemy_car['position']['y'], enemy_car['position']['z']\n",
    "    except KeyError:\n",
    "        return 0.0\n",
    "\n",
    "    dx, dy, dz = ex - sx, ey - sy, ez - sz\n",
    "    distance = math.sqrt(dx*dx + dy*dy + dz*dz)\n",
    "\n",
    "    if 'prev_distance' not in globals():\n",
    "        prev_distance = distance\n",
    "\n",
    "    # ========== 核心奖励 ==========\n",
    "    delta = prev_distance - distance\n",
    "\n",
    "    # 非常重要：放大\n",
    "    reward = 0.05 * delta    # 每接近 1 m +0.05\n",
    "\n",
    "    # 防止原地摆烂\n",
    "    reward -= 0.001          # 每步轻惩罚\n",
    "\n",
    "    # 终止奖励\n",
    "    if distance < 50:\n",
    "        reward += 500.0\n",
    "\n",
    "    prev_distance = distance\n",
    "    return float(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_reward_v2(state: dict) -> float:\n",
    "    global prev_distance\n",
    "\n",
    "    if 'carInfo' not in state or len(state['carInfo']) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # 区分敌我\n",
    "    if state['carInfo'][0]['baseInfo']['side'] == 1:\n",
    "        self_car = state['carInfo'][0]\n",
    "        enemy_car = state['carInfo'][1]\n",
    "    else:\n",
    "        self_car = state['carInfo'][1]\n",
    "        enemy_car = state['carInfo'][0]\n",
    "\n",
    "    try:\n",
    "        sx, sy, sz = self_car['position']['x'], self_car['position']['y'], self_car['position']['z']\n",
    "        ex, ey, ez = enemy_car['position']['x'], enemy_car['position']['y'], enemy_car['position']['z']\n",
    "        vx, vy, vz = self_car['velocity']['x'], self_car['velocity']['y'], self_car['velocity']['z']\n",
    "    except KeyError:\n",
    "        return 0.0\n",
    "\n",
    "    dx, dy, dz = ex - sx, ey - sy, ez - sz\n",
    "    distance = math.sqrt(dx*dx + dy*dy + dz*dz)\n",
    "    speed = math.sqrt(vx*vx + vy*vy + vz*vz)\n",
    "\n",
    "    if 'prev_distance' not in globals():\n",
    "        prev_distance = distance\n",
    "\n",
    "    # ========== 1) 距离进度奖励，核心奖励 ==========\n",
    "    delta = prev_distance - distance\n",
    "    reward = 0.05 * delta    # 每接近 1 m +0.05\n",
    "    # 2) LOS航向奖励（辅助，别太大）\n",
    "    # ======================================================\n",
    "    r_los = 0.0\n",
    "    if speed > 0.5:\n",
    "        to_target = np.array([dx, dy, dz], dtype=np.float32) / distance\n",
    "        vel_dir = np.array([vx, vy, vz], dtype=np.float32) / (speed + 1e-6)\n",
    "        cos_theta = float(np.clip(np.dot(to_target, vel_dir), -1.0, 1.0))\n",
    "        # 只奖励朝向目标（cos>0），避免“背对目标也拿负奖励把梯度搞乱”\n",
    "        r_los = 0.2 * max(0.0, cos_theta)\n",
    "        \n",
    "    # 3) 速度 shaping（远处快，近处别太快）\n",
    "    # ======================================================\n",
    "    if distance > 1500:\n",
    "        desired_speed = 18.0\n",
    "    elif distance > 500:\n",
    "        desired_speed = 14.0\n",
    "    elif distance > 150:\n",
    "        desired_speed = 9.0\n",
    "    else:\n",
    "        desired_speed = 6.0\n",
    "    # 让速度靠近 desired_speed（轻一点，别压过主目标）\n",
    "    r_speed = -0.02 * abs(speed - desired_speed)\n",
    "\n",
    "    # 防止原地摆烂\n",
    "    reward -= 0.001          # 每步轻惩罚\n",
    "\n",
    "    # 终止奖励\n",
    "    if distance < 50:\n",
    "        reward += 500.0\n",
    "\n",
    "    reward += r_los + r_speed \n",
    "    prev_distance = distance\n",
    "    return float(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_Termination(state:Dict[str, any]) -> bool:\n",
    "    #example\n",
    "    if state['carInfo'][0]['baseInfo']['side'] == 1:\n",
    "        self_pos = state['carInfo'][0]['position']\n",
    "        enemy_pos = state['carInfo'][1]['position']\n",
    "    elif state['carInfo'][1]['baseInfo']['side'] == 2:\n",
    "        self_pos = state['carInfo'][1]['position']\n",
    "        enemy_pos = state['carInfo'][0]['position']\n",
    "    \n",
    "    try:\n",
    "        # 获取坐标值\n",
    "        x1 = self_pos['x']\n",
    "        y1 = self_pos['y']\n",
    "        z1 = self_pos['z']\n",
    "        x2 = enemy_pos['x']\n",
    "        y2 = enemy_pos['y']\n",
    "        z2 = enemy_pos['z']\n",
    "        # 计算距离\n",
    "        distance = math.sqrt((x2 - x1)**2 + (y2 - y1)**2 + (z2 - z1)**2)\n",
    "    except (KeyError, ValueError, TypeError) as e:\n",
    "        # 如果数据格式有问题，不终止\n",
    "        print(f\"计算距离时出错: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # 距离小于10米，终止\n",
    "    if distance < 50:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_算法参数配置_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "sys.path.append(os.path.abspath(os.getcwd()))\n",
    "num_agents = 1\n",
    "num_enemies = 1\n",
    "episode_length = 100\n",
    "save_interval = 1000\n",
    "log_interval = 10\n",
    "model_dir = (\n",
    "        Path(os.path.dirname(os.path.dirname(os.getcwd())) + \"/results\")\n",
    "        /\"save\"\n",
    "    )\n",
    "all_args = {\n",
    "    \"algorithm_name\": \"ppo\",\n",
    "    \"use_recurrent_policy\": False,\n",
    "    \"use_naive_recurrent_policy\": False,\n",
    "    \"share_policy\": True,\n",
    "    \"use_wandb\": True,\n",
    "    \"seed\": 0,\n",
    "    \"use_centralized_V\": True,\n",
    "    \"use_linear_lr_decay\": True,\n",
    "    \"hidden_size\": 16,\n",
    "    \"recurrent_N\": 1,\n",
    "    \"act_space\": 2,\n",
    "    \"obs_space\": 12,\n",
    "    \"shared_obs_space\": 12*num_agents,\n",
    "    \"model_dir\": None,\n",
    "    \"episode_length\": episode_length,\n",
    "    \"gamma\": 0.98,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"use_gae\": True,\n",
    "    \"clip_param\": 0.2,\n",
    "    \"ppo_epoch\": 15,\n",
    "    \"num_mini_batch\": 1,\n",
    "    \"data_chunk_length\": 10,\n",
    "    \"value_loss_coef\": 0.5,\n",
    "    \"entropy_coef\": 0.01,\n",
    "    \"max_grad_norm\": 10.0,\n",
    "    \"huber_delta\": 10.0,\n",
    "    \"use_max_grad_norm\": True,\n",
    "    \"use_clipped_value_loss\": True,\n",
    "    \"use_huber_loss\": True,\n",
    "    \"use_popart\": True,\n",
    "    \"use_valuenorm\": False,\n",
    "    \"use_value_active_masks\": True,\n",
    "    \"use_policy_active_masks\": True,\n",
    "    \"lr\": 7e-5,\n",
    "    \"critic_lr\": 7e-4,\n",
    "    \"opti_eps\": 1e-5,\n",
    "    \"weight_decay\": 0,\n",
    "    \"gain\": 0.01,\n",
    "    \"use_orthogonal\": True,\n",
    "    \"use_feature_normalization\": True,\n",
    "    \"use_ReLU\": False,\n",
    "    \"stacked_frames\": 1,\n",
    "    \"layer_N\": 1,\n",
    "    \"n_rollout_threads\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "run_dir = (\n",
    "        Path(os.path.dirname(os.path.dirname(os.getcwd())) + \"/results\")\n",
    "        / all_args[\"algorithm_name\"]\n",
    "    )\n",
    "config = {\n",
    "    \"all_args\": all_args,\n",
    "    \"num_agents\": num_agents,\n",
    "    \"num_enemies\":num_enemies,\n",
    "    \"device\": device,\n",
    "    \"run_dir\": run_dir\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*W&B记录训练日志*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb.init() called while a run is active and reinit is set to 'return_previous', so returning the previous run."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if all_args[\"use_wandb\"]:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        # for supercloud when no internet_connection\n",
    "        # if not connected_to_internet():\n",
    "        #     import json\n",
    "\n",
    "        #     # save a json file with your wandb api key in your\n",
    "        #     # home folder as {'my_wandb_api_key': 'INSERT API HERE'}\n",
    "        #     # NOTE this is only for running on systems without internet access\n",
    "        #     # have to run `wandb sync wandb/run_name` to sync logs to wandboard\n",
    "        #     with open(os.path.dirname(os.path.dirname(os.getcwd())) + \"/keys.json\") as json_file:\n",
    "        #         key = json.load(json_file)\n",
    "        #         my_wandb_api_key = key[\"my_wandb_api_key\"]  # NOTE change here as well\n",
    "        #     os.environ[\"WANDB_API_KEY\"] = my_wandb_api_key\n",
    "        #     os.environ[\"WANDB_MODE\"] = \"dryrun\"\n",
    "        #     os.environ[\"WANDB_SAVE_CODE\"] = \"true\"\n",
    "\n",
    "        # print_box(\"Creating wandboard...\")\n",
    "        run_name = f\"{all_args['algorithm_name']}_seed{all_args['seed']}_{timestamp}\"\n",
    "        run = wandb.init(\n",
    "            config=all_args,\n",
    "            project=\"simplecq\",\n",
    "            # project=all_args.env_name,\n",
    "            # entity=\"cc\",\n",
    "            notes=socket.gethostname(),\n",
    "            name=run_name,\n",
    "            # group=all_args.scenario_name,\n",
    "            dir=str(run_dir),\n",
    "            # job_type=\"training\",\n",
    "            reinit='return_previous',\n",
    "        )\n",
    "        \n",
    "setproctitle.setproctitle(\n",
    "        str(all_args[\"algorithm_name\"])\n",
    "        + \"@\"\n",
    "        + str(\"wsbbuaa\")\n",
    "    )\n",
    "\n",
    "# seed\n",
    "torch.manual_seed(all_args[\"seed\"])\n",
    "torch.cuda.manual_seed_all(all_args[\"seed\"])\n",
    "np.random.seed(all_args[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO参数设置\n",
    "state_dim = 12  # 状态维度\n",
    "action_dim = 2  # 动作维度\n",
    "lr_actor = 3e-4\n",
    "lr_critic = 3e-4  # Critic学习率\n",
    "gamma = 0.995  # 折扣因子\n",
    "K_epochs = 10  # 每次更新的训练轮数\n",
    "eps_clipping = 0.2  # PPO裁剪参数\n",
    "is_continuous_action_space = True  # 连续动作空间\n",
    "\n",
    "# 初始化PPO\n",
    "ppo_agent = PPO(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    lr_actor=lr_actor,\n",
    "    lr_critic=lr_critic,\n",
    "    gamma=gamma,\n",
    "    K_epochs=K_epochs,\n",
    "    eps_clipping=eps_clipping,\n",
    "    is_continuous_action_space=is_continuous_action_space\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*端口及输出类型指定*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 40029\n",
    "outputs_type = {\n",
    "    \"targetDir\": \"float\",\n",
    "    \"targetVel\": \"float\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(port=port, \n",
    "              outputs_type=outputs_type,\n",
    "              process_input=parse_Input,\n",
    "              process_output=parse_Output,\n",
    "              reward_func=cal_reward_v1_5,\n",
    "              end_func=cal_Termination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_state(raw_state: Dict[str, any]) -> tuple:\n",
    "    \"\"\"\n",
    "    将原始状态字典转换为状态向量\n",
    "    \n",
    "    根据文档设计的状态空间：\n",
    "    [d, cos(θ_rel), sin(θ_rel), v_self, v_enemy, cos(Δψ), sin(Δψ), d_dot, ...]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 提取车辆信息\n",
    "        if raw_state['carInfo'][0]['baseInfo']['side'] == 1:\n",
    "            self_car = raw_state['carInfo'][0]\n",
    "            enemy_car = raw_state['carInfo'][1]\n",
    "        else:\n",
    "            self_car = raw_state['carInfo'][1]\n",
    "            enemy_car = raw_state['carInfo'][0]\n",
    "        \n",
    "        # 提取位置和速度\n",
    "        self_pos = self_car['position']\n",
    "        enemy_pos = enemy_car['position']\n",
    "        self_vel = self_car['velocity']\n",
    "        enemy_vel = enemy_car['velocity']\n",
    "        \n",
    "        # 计算相对位置向量\n",
    "        dx = enemy_pos['x'] - self_pos['x']\n",
    "        dy = enemy_pos['y'] - self_pos['y']\n",
    "        dz = enemy_pos['z'] - self_pos['z']\n",
    "        \n",
    "        # 距离\n",
    "        distance = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "        \n",
    "        # 相对角度（目标相对于我车航向）\n",
    "        # 假设有航向信息\n",
    "        self_heading = self_car.get('heading', 0.0)  # 需要从状态中获取\n",
    "        enemy_heading = enemy_car.get('heading', 0.0)\n",
    "        \n",
    "        # 相对角度计算\n",
    "        target_angle = np.arctan2(dy, dx)\n",
    "        theta_rel = target_angle - self_heading\n",
    "        # 归一化到 [-π, π]\n",
    "        theta_rel = (theta_rel + np.pi) % (2 * np.pi) - np.pi\n",
    "        \n",
    "        # 航向差\n",
    "        delta_psi = enemy_heading - self_heading\n",
    "        delta_psi = (delta_psi + np.pi) % (2 * np.pi) - np.pi\n",
    "        \n",
    "        # 速度信息\n",
    "        v_self = np.sqrt(self_vel['x']**2 + self_vel['y']**2 + self_vel['z']**2)\n",
    "        v_enemy = np.sqrt(enemy_vel['x']**2 + enemy_vel['y']**2 + enemy_vel['z']**2)\n",
    "        \n",
    "        # 相对速度\n",
    "        v_rel_x = enemy_vel['x'] - self_vel['x']\n",
    "        v_rel_y = enemy_vel['y'] - self_vel['y']\n",
    "        \n",
    "        # 接近率\n",
    "        if distance > 0.001:\n",
    "            d_dot = -(dx * v_rel_x + dy * v_rel_y) / distance\n",
    "        else:\n",
    "            d_dot = 0.0\n",
    "        \n",
    "        # 构建状态向量（根据文档设计）\n",
    "        state_vector = np.array([\n",
    "            distance,                   # 相对距离\n",
    "            np.cos(theta_rel),           # 相对角度的余弦\n",
    "            np.sin(theta_rel),           # 相对角度的正弦\n",
    "            v_self,                     # 自身速度\n",
    "            v_enemy,                    # 目标速度\n",
    "            np.cos(delta_psi),          # 航向差余弦\n",
    "            np.sin(delta_psi),          # 航向差正弦\n",
    "            d_dot,                      # 接近率\n",
    "            v_rel_x,                     # 相对速度x\n",
    "            v_rel_y,                    # 相对速度y\n",
    "            dx/distance if distance > 0 else 0,  # 归一化相对位置x\n",
    "            dy/distance if distance > 0 else 0,  # 归一化相对位置y\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        # 状态归一化\n",
    "        normalized_state_vector = normalize_state(state_vector.copy())\n",
    "        \n",
    "        return state_vector, normalized_state_vector\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"状态处理错误: {e}\")\n",
    "        # 返回零状态\n",
    "        zero_vector = np.zeros(12, dtype=np.float32)\n",
    "        return zero_vector, zero_vector\n",
    "    \n",
    "def normalize_state(state: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    状态归一化\n",
    "    \"\"\"\n",
    "    if len(state) != 12:\n",
    "        raise ValueError(f\"状态向量维度应为12，但得到{len(state)}\")\n",
    "    \n",
    "    normalized = state.copy()\n",
    "    \n",
    "    # 1. 相对距离 [0] - 范围: [0, +∞)\n",
    "    # 假设最大观测距离为10000米，使用对数压缩处理大范围距离\n",
    "    if state[0] > 0:\n",
    "        normalized[0] = np.log1p(state[0]) / np.log1p(10000)  # 对数归一化\n",
    "    else:\n",
    "        normalized[0] = 0.0\n",
    "    \n",
    "    # 2. 相对角度余弦 [1] - 范围: [-1, 1]，已经是归一化的\n",
    "    # 不需要额外处理\n",
    "    \n",
    "    # 3. 相对角度正弦 [2] - 范围: [-1, 1]，已经是归一化的\n",
    "    # 不需要额外处理\n",
    "    \n",
    "    # 4. 自身速度 [3] - 范围: [0, +∞)\n",
    "    # 假设最大速度为20 m/s\n",
    "    normalized[3] = np.tanh(state[3] / 20.0)  # 使用tanh限制在[-1,1]\n",
    "    \n",
    "    # 5. 目标速度 [4] - 范围: [0, +∞)\n",
    "    normalized[4] = np.tanh(state[4] / 20.0)  # 使用tanh限制在[-1,1]\n",
    "    \n",
    "    # 6. 航向差余弦 [5] - 范围: [-1, 1]，已经是归一化的\n",
    "    # 不需要额外处理\n",
    "    \n",
    "    # 7. 航向差正弦 [6] - 范围: [-1, 1]，已经是归一化的\n",
    "    # 不需要额外处理\n",
    "    \n",
    "    # 8. 接近率 [7] - 范围: (-∞, +∞)\n",
    "    # 接近率可能很大，使用tanh压缩\n",
    "    normalized[7] = np.tanh(state[7] / 20.0)  # 除以20进行缩放\n",
    "    \n",
    "    # 9. 相对速度x [8] - 范围: (-∞, +∞)\n",
    "    normalized[8] = np.tanh(state[8] / 20.0)  # 假设最大相对速度20 m/s\n",
    "    \n",
    "    # 10. 相对速度y [9] - 范围: (-∞, +∞)\n",
    "    normalized[9] = np.tanh(state[9] / 20.0)  # 假设最大相对速度20 m/s\n",
    "    \n",
    "    # 11. 归一化相对位置x [10] - 范围: [-1, 1]，已经是归一化的\n",
    "    # 确保在有效范围内\n",
    "    normalized[10] = np.clip(state[10], -1.0, 1.0)\n",
    "    \n",
    "    # 12. 归一化相对位置y [11] - 范围: [-1, 1]，已经是归一化的\n",
    "    normalized[11] = np.clip(state[11], -1.0, 1.0)\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*训练函数*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env: Agent, episodes, enable_log=True):\n",
    "    total_steps = 0\n",
    "    decision_interval = 10\n",
    "    max_steps_per_episode = 20000\n",
    "    Win = 0\n",
    "    best_sr = 0\n",
    "    episode_rewards_history = []\n",
    "    success_rate_history = []\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_dir = \"D:/A_code/simplecq/record\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        episode_reward = 0.0\n",
    "        episode_steps = 0\n",
    "\n",
    "        # 清空 PPO buffer\n",
    "        ppo_agent.buffer.clear()\n",
    "\n",
    "        # reset env\n",
    "        raw_state = env.reset()\n",
    "        raw_state_dic = parse_Output(raw_state)\n",
    "        log_state, current_state = process_raw_state(raw_state_dic)\n",
    "        episode_min_distance = float(\"inf\")\n",
    "        episode_last_distance = None\n",
    "        # === 延迟一拍相关变量 ===\n",
    "        acc_reward = 0.0\n",
    "\n",
    "        # === 日志缓存 ===\n",
    "        episode_logs = []\n",
    "        # 初始动作（占位）\n",
    "        action = np.array([0.0, 0.0])\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            # # ---------- 低频决策 ----------\n",
    "            if step % decision_interval == 0:\n",
    "                action = ppo_agent.select_action(current_state)\n",
    "\n",
    "                # 记录日志（只在决策帧）\n",
    "                raw_v, raw_d = float(action[0]), float(action[1])\n",
    "                velocity = (raw_v + 1) * 10\n",
    "                direction = raw_d * np.pi\n",
    "                episode_logs.append(\n",
    "                    f\"[{episode}] {' '.join(map(str, log_state))} {[velocity, direction]}\"\n",
    "                )\n",
    "\n",
    "            # ---------- 环境推进 ----------\n",
    "            next_raw_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            if next_raw_state == '':\n",
    "                # step 无效，直接跳过（不结算 reward）\n",
    "                continue\n",
    "\n",
    "            # ====== 计算 distance（只用 state，不碰 reward） ======\n",
    "            state_dict = next_raw_state\n",
    "\n",
    "            if 'carInfo' in state_dict and len(state_dict['carInfo']) >= 2:\n",
    "                if state_dict['carInfo'][0]['baseInfo']['side'] == 1:\n",
    "                    self_car = state_dict['carInfo'][0]\n",
    "                    enemy_car = state_dict['carInfo'][1]\n",
    "                else:\n",
    "                    self_car = state_dict['carInfo'][1]\n",
    "                    enemy_car = state_dict['carInfo'][0]\n",
    "\n",
    "                dx = enemy_car['position']['x'] - self_car['position']['x']\n",
    "                dy = enemy_car['position']['y'] - self_car['position']['y']\n",
    "                dz = enemy_car['position']['z'] - self_car['position']['z']\n",
    "\n",
    "                distance = (dx*dx + dy*dy + dz*dz) ** 0.5\n",
    "\n",
    "                episode_min_distance = min(episode_min_distance, distance)\n",
    "                episode_last_distance = distance\n",
    "            # ---------- reward 延迟一拍结算 ----------\n",
    "            acc_reward += reward\n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "            total_steps += 1\n",
    "            # ---------- 状态更新 ----------\n",
    "            log_state, next_state = process_raw_state(next_raw_state)\n",
    "            current_state = next_state\n",
    "\n",
    "            if done:\n",
    "                Win += 1\n",
    "                ppo_agent.buffer.rewards.append(acc_reward)\n",
    "                ppo_agent.buffer.is_terminals.append(True)\n",
    "                acc_reward = 0.0\n",
    "                break\n",
    "            # 非终止：每到达一个决策段末尾就写入一次（标 terminal=False）\n",
    "            if (step + 1) % decision_interval == 0:\n",
    "                ppo_agent.buffer.rewards.append(acc_reward)\n",
    "                ppo_agent.buffer.is_terminals.append(False)\n",
    "                acc_reward = 0.0\n",
    "\n",
    "\n",
    "        # ---------- PPO 更新 ----------\n",
    "        ppo_agent.update()\n",
    "\n",
    "        # ---------- 日志写文件（episode 级） ----------\n",
    "        # log_path = f\"{log_dir}/action0112_{timestamp}.txt\"\n",
    "        # with open(log_path, \"a\") as f:\n",
    "        #     for line in episode_logs:\n",
    "        #         f.write(line + \"\\n\")\n",
    "\n",
    "        # ---------- 统计 ----------\n",
    "        episode_rewards_history.append(episode_reward)\n",
    "        success_rate = Win / (episode + 1)\n",
    "        success_rate_history.append(success_rate)\n",
    "\n",
    "        if all_args.get(\"use_wandb\", False):\n",
    "            wandb.log({\n",
    "                \"episode_min_distance\": episode_min_distance,\n",
    "                \"episode_last_distance\": episode_last_distance,\n",
    "                \"episode\": episode,\n",
    "                \"episode_reward\": episode_reward,\n",
    "                \"episode_length\": episode_steps,\n",
    "                \"success_rate\": success_rate,\n",
    "                \"win_count\": Win,\n",
    "                \"total_steps\": total_steps,\n",
    "                \"avg_reward_100\": np.mean(episode_rewards_history[-100:])\n",
    "            }, step=episode)\n",
    "\n",
    "        # ---------- 保存模型 ----------\n",
    "        save_cooldown = 20        # 至少间隔 20 个 episode 才允许再存\n",
    "        min_improve = 0.02        # 至少提升 2% 才存\n",
    "        last_best_save_ep = -10**9\n",
    "        if (success_rate > best_sr + min_improve) and (episode - last_best_save_ep >= save_cooldown):\n",
    "            best_sr = success_rate\n",
    "            last_best_save_ep = episode\n",
    "            os.makedirs(str(model_dir), exist_ok=True)\n",
    "            ppo_agent.save(f\"{model_dir}/ppo_best0112_{episode}.pth\")\n",
    "        # if episode % save_interval == 0 or episode == episodes - 1:\n",
    "        #     ppo_agent.save(f\"{model_dir}/ppo_model_{episode}.pth\")\n",
    "\n",
    "        # ---------- 打印 ----------\n",
    "        if episode % log_interval == 0:\n",
    "            avg_reward = np.mean(episode_rewards_history[-100:])\n",
    "            print(f\"回合 {episode}/{episodes}\")\n",
    "            print(f\"  累计奖励: {episode_reward:.2f}\")\n",
    "            print(f\"  步数: {episode_steps}\")\n",
    "            print(f\"  平均奖励(100): {avg_reward:.2f}\")\n",
    "            print(f\"  成功率: {success_rate:.3f}\")\n",
    "\n",
    "        if episode % 100 == 0 and enable_log:\n",
    "            print(f\"已完成：{episode / episodes * 100:.1f}%\")\n",
    "\n",
    "    print(f\"训练结束。成功：{Win} 次\")\n",
    "\n",
    "    if all_args.get(\"use_wandb\", False):\n",
    "        run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*运行训练*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态处理错误: list index out of range\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "[WinError 10054] 远程主机强迫关闭了一个现有的连接。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 49\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(env, episodes, enable_log)\u001b[0m\n\u001b[0;32m     44\u001b[0m     episode_logs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;250m \u001b[39mlog_state))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[velocity,\u001b[38;5;250m \u001b[39mdirection]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m     )\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# ---------- 环境推进 ----------\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m next_raw_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_raw_state \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# step 无效，直接跳过（不结算 reward）\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32md:\\codeproject\\simplecq\\config\\gym_interface.py:78\u001b[0m, in \u001b[0;36mAgent.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# self._send(self.process_input(action))\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlink\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_input(action)\u001b[38;5;241m.\u001b[39mencode() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m s_tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m#如果s_tmp为'[]', 说明模型已经结束, 直接返回None\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s_tmp) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\codeproject\\simplecq\\config\\gym_interface.py:98\u001b[0m, in \u001b[0;36mAgent._recv\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     96\u001b[0m s: \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 98\u001b[0m     s \u001b[38;5;241m=\u001b[39m s \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlink\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28meval\u001b[39m(s\u001b[38;5;241m.\u001b[39mdecode())\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] 远程主机强迫关闭了一个现有的连接。"
     ]
    }
   ],
   "source": [
    "train(agent, 500)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simplecq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
